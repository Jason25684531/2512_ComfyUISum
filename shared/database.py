"""
Database Module for Studio Core
æä¾› MySQL é€£æ¥æ± ã€ORM æ¨¡å‹ (User, Job) å’Œè³‡æ–™åº«æ“ä½œ

Phase: Member System Beta
- æ–°å¢ User æ¨¡å‹ (UserMixin)
- æ–°å¢ Job æ¨¡å‹ (FK: user_id)
- ç§»é™¤ output_pathï¼Œæ”¹ç”¨ ID æ¨å°æª”å
"""
import os
import logging
from typing import Optional, List, Dict, Any
from datetime import datetime, timezone

# MySQL Connector (é€£æ¥æ± )
import mysql.connector
from mysql.connector import pooling, Error

# SQLAlchemy ORM
from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, Boolean, ForeignKey
from sqlalchemy.orm import declarative_base, relationship, sessionmaker, scoped_session
from sqlalchemy.dialects.mysql import JSON

# Flask-Login
from flask_login import UserMixin

logger = logging.getLogger(__name__)

# ===========================================
# SQLAlchemy Base å’Œ Engine
# ===========================================
Base = declarative_base()

# å…¨å±€ Session å’Œ Engine (å»¶é²åˆå§‹åŒ–)
_engine = None
_session_factory = None


def get_db_engine(db_url: Optional[str] = None):
    """ç²å–æˆ–å»ºç«‹ SQLAlchemy Engine"""
    global _engine
    if _engine is None:
        if db_url is None:
            # å¾ç’°å¢ƒè®Šæ•¸å»ºç«‹ URL
            host = os.getenv("DB_HOST", "localhost")
            port = os.getenv("DB_PORT", "3306")
            user = os.getenv("DB_USER", "studio_user")
            password = os.getenv("DB_PASSWORD", "studio_password")
            database = os.getenv("DB_NAME", "studio_db")
            db_url = f"mysql+mysqlconnector://{user}:{password}@{host}:{port}/{database}"
        
        _engine = create_engine(
            db_url,
            pool_size=5,
            max_overflow=10,
            pool_recycle=3600,
            echo=False
        )
        logger.info(f"âœ“ SQLAlchemy Engine å»ºç«‹æˆåŠŸ")
    return _engine


def get_db_session():
    """ç²å– Scoped Session"""
    global _session_factory
    if _session_factory is None:
        engine = get_db_engine()
        _session_factory = scoped_session(sessionmaker(bind=engine))
    return _session_factory()


def init_db():
    """åˆå§‹åŒ–è³‡æ–™åº«è¡¨æ ¼ (ä½¿ç”¨ ORM å»ºç«‹)"""
    engine = get_db_engine()
    Base.metadata.create_all(engine)
    logger.info("âœ“ SQLAlchemy ORM è¡¨æ ¼åˆå§‹åŒ–å®Œæˆ")


# ===========================================
# ORM Models
# ===========================================

class User(UserMixin, Base):
    """
    ç”¨æˆ¶æ¨¡å‹ - æ”¯æ´ Flask-Login
    
    Attributes:
        id: ç”¨æˆ¶ ID (PK)
        email: ç™»å…¥å¸³è™Ÿ (Unique)
        password_hash: Bcrypt åŠ å¯†å¯†ç¢¼
        name: é¡¯ç¤ºæš±ç¨±
        role: æ¬Šé™ (member/admin)
        created_at: è¨»å†Šæ™‚é–“
    """
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    email = Column(String(255), unique=True, nullable=False, index=True)
    password_hash = Column(String(255), nullable=False)
    name = Column(String(50), nullable=False)
    role = Column(String(20), default='member')
    created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    
    # Relationship: User -> Jobs (ä¸€å°å¤š)
    jobs = relationship("Job", back_populates="user", lazy="dynamic")
    
    def to_dict(self):
        """è½‰æ›ç‚ºå­—å…¸ï¼ˆAPI å›æ‡‰ç”¨ï¼‰"""
        return {
            "id": self.id,
            "email": self.email,
            "name": self.name,
            "role": self.role,
            "created_at": self.created_at.isoformat() if self.created_at else None
        }


class Job(Base):
    """
    ä»»å‹™æ¨¡å‹ - ç®—åœ–ä»»å‹™è¨˜éŒ„
    
    Attributes:
        id: ä»»å‹™ ID (PK, UUID å­—ä¸²)
        user_id: ç”¨æˆ¶ ID (FK, å¯ç‚ºç©ºä»¥ç›¸å®¹èˆŠè³‡æ–™)
        prompt: æç¤ºè©
        workflow_name: å·¥ä½œæµåç¨±
        workflow_data: ComfyUI å®Œæ•´åƒæ•¸ (JSON)
        model: æ¨¡å‹åç¨±
        aspect_ratio: åœ–ç‰‡æ¯”ä¾‹
        batch_size: æ‰¹æ¬¡å¤§å°
        seed: éš¨æ©Ÿç¨®å­
        status: ä»»å‹™ç‹€æ…‹
        input_audio_path: è¼¸å…¥éŸ³è¨Šæª”å
        created_at: å»ºç«‹æ™‚é–“
        updated_at: æ›´æ–°æ™‚é–“
        deleted_at: è»Ÿåˆªé™¤æ™‚é–“ (Nullable)
    """
    __tablename__ = 'jobs'
    
    id = Column(String(36), primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id', ondelete='SET NULL'), nullable=True, index=True)
    prompt = Column(Text, nullable=True)
    workflow_name = Column(String(50), nullable=True)  # å·¥ä½œæµåç¨±
    workflow_data = Column(JSON, nullable=True)  # å®Œæ•´ ComfyUI åƒæ•¸
    model = Column(String(100), nullable=True)
    aspect_ratio = Column(String(10), nullable=True)
    batch_size = Column(Integer, default=1)
    seed = Column(Integer, default=-1)
    status = Column(String(20), default='queued')
    input_audio_path = Column(String(255), nullable=True)
    created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    updated_at = Column(DateTime, default=lambda: datetime.now(timezone.utc), onupdate=lambda: datetime.now(timezone.utc))
    deleted_at = Column(DateTime, nullable=True)
    
    # ç›¸å®¹èˆŠæ¬„ä½ (æ¨™è¨˜ç‚ºæ£„ç”¨ï¼Œä¿ç•™ä»¥é¿å…é·ç§»éŒ¯èª¤)
    is_deleted = Column(Boolean, default=False)
    
    # Relationship: Job -> User (å¤šå°ä¸€)
    user = relationship("User", back_populates="jobs")
    
    def to_dict(self):
        """è½‰æ›ç‚ºå­—å…¸ï¼ˆAPI å›æ‡‰ç”¨ï¼‰"""
        return {
            "id": self.id,
            "user_id": self.user_id,
            "prompt": self.prompt,
            "workflow": self.workflow_name,
            "model": self.model,
            "aspect_ratio": self.aspect_ratio,
            "batch_size": self.batch_size,
            "seed": self.seed,
            "status": self.status,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None
        }


# ===========================================
# åŸæœ‰ Database é¡ (é€£æ¥æ±  + åŸç”Ÿ SQL)
# ä¿ç•™ä»¥ç›¸å®¹ç¾æœ‰ç¨‹å¼ç¢¼
# ===========================================

class Database:
    """MySQL è³‡æ–™åº«ç®¡ç†é¡ (ä½¿ç”¨é€£æ¥æ± )"""
    
    def __init__(
        self,
        host: str,
        port: int,
        user: str,
        password: str,
        database: str,
        pool_name: str = "studio_pool",
        pool_size: int = 5
    ):
        """
        åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥æ± 
        
        Args:
            host: MySQL ä¸»æ©Ÿä½å€
            port: MySQL ç«¯å£
            user: ç”¨æˆ¶å
            password: å¯†ç¢¼
            database: è³‡æ–™åº«åç¨±
            pool_name: é€£æ¥æ± åç¨±
            pool_size: é€£æ¥æ± å¤§å°
        """
        self.config = {
            "host": host,
            "port": port,
            "user": user,
            "password": password,
            "database": database,
            "pool_name": pool_name,
            "pool_size": pool_size,
            "pool_reset_session": True,
        }
        
        try:
            self.pool = pooling.MySQLConnectionPool(**self.config)
            logger.info(f"âœ“ MySQL é€£æ¥æ± å»ºç«‹æˆåŠŸ: {host}:{port}/{database}")
            self._init_schema()
        except Error as e:
            logger.error(f"âœ— MySQL é€£æ¥æ± å»ºç«‹å¤±æ•—: {e}")
            raise
    
    def _init_schema(self):
        """åˆå§‹åŒ–è³‡æ–™åº« Schema - å»ºç«‹ users, jobs, user_mapping è¡¨"""
        # æ–°å¢ users è¡¨
        create_users_table_sql = """
        CREATE TABLE IF NOT EXISTS users (
            id INT PRIMARY KEY AUTO_INCREMENT,
            email VARCHAR(255) UNIQUE NOT NULL,
            password_hash VARCHAR(255) NOT NULL,
            name VARCHAR(50) NOT NULL,
            role VARCHAR(20) DEFAULT 'member',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            INDEX idx_email (email)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        """
        
        # æ›´æ–° jobs è¡¨ (æ–°å¢ user_id, workflow_data, deleted_at)
        create_jobs_table_sql = """
        CREATE TABLE IF NOT EXISTS jobs (
            id VARCHAR(36) PRIMARY KEY,
            user_id INT DEFAULT NULL,
            prompt TEXT,
            workflow_name VARCHAR(50),
            workflow_data JSON,
            model VARCHAR(100),
            aspect_ratio VARCHAR(10),
            batch_size INT DEFAULT 1,
            seed INT DEFAULT -1,
            status VARCHAR(20),
            input_audio_path VARCHAR(255) DEFAULT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
            deleted_at TIMESTAMP NULL DEFAULT NULL,
            is_deleted BOOLEAN DEFAULT FALSE,
            INDEX idx_user_id (user_id),
            INDEX idx_status (status),
            INDEX idx_created_at (created_at),
            INDEX idx_deleted_at (deleted_at),
            CONSTRAINT fk_jobs_user FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE SET NULL
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        """
        
        create_user_mapping_table_sql = """
        CREATE TABLE IF NOT EXISTS user_mapping (
            id INT PRIMARY KEY AUTO_INCREMENT,
            ip_address VARCHAR(45) UNIQUE NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            last_active TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
            INDEX idx_ip (ip_address),
            INDEX idx_last_active (last_active)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
        """
        
        try:
            conn = self.pool.get_connection()
            cursor = conn.cursor()
            
            # å…ˆå»ºç«‹ users è¡¨ (å› ç‚º jobs æœ‰ FK ä¾è³´)
            cursor.execute(create_users_table_sql)
            cursor.execute(create_jobs_table_sql)
            cursor.execute(create_user_mapping_table_sql)
            conn.commit()
            logger.info("âœ“ Users, Jobs, user_mapping è¡¨åˆå§‹åŒ–æˆåŠŸ")
        except Error as e:
            logger.error(f"âœ— å»ºç«‹è¡¨å¤±æ•—: {e}")
        finally:
            if conn.is_connected():
                cursor.close()
                conn.close()
    
    def insert_job(
        self,
        job_id: str,
        prompt: str,
        workflow: str,
        model: str,
        aspect_ratio: str = "1:1",
        batch_size: int = 1,
        seed: int = -1,
        status: str = "queued",
        input_audio_path: Optional[str] = None,
        user_id: Optional[int] = None,
        workflow_data: Optional[dict] = None
    ) -> bool:
        """
        æ’å…¥æ–°ä»»å‹™è¨˜éŒ„
        
        Args:
            job_id: ä»»å‹™ ID (UUID)
            prompt: æç¤ºè©
            workflow: å·¥ä½œæµåç¨±
            model: æ¨¡å‹åç¨±
            aspect_ratio: åœ–ç‰‡æ¯”ä¾‹
            batch_size: æ‰¹æ¬¡å¤§å°
            seed: éš¨æ©Ÿç¨®å­
            status: ä»»å‹™ç‹€æ…‹
            input_audio_path: è¼¸å…¥éŸ³è¨Šæª”å
            user_id: ç”¨æˆ¶ ID (Member System)
            workflow_data: å®Œæ•´å·¥ä½œæµåƒæ•¸ (JSON)
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        import json
        
        sql = """
        INSERT INTO jobs (id, user_id, prompt, workflow_name, workflow_data, model, aspect_ratio, batch_size, seed, status, input_audio_path)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        workflow_json = json.dumps(workflow_data) if workflow_data else None
        
        try:
            conn = self.pool.get_connection()
            cursor = conn.cursor()
            cursor.execute(sql, (job_id, user_id, prompt, workflow, workflow_json, model, aspect_ratio, batch_size, seed, status, input_audio_path))
            conn.commit()
            logger.info(f"âœ“ ä»»å‹™è¨˜éŒ„æ’å…¥æˆåŠŸ: {job_id}" + (f" (User: {user_id})" if user_id else ""))
            return True
        except Error as e:
            logger.error(f"âœ— æ’å…¥ä»»å‹™å¤±æ•—: {e}")
            return False
        finally:
            if conn.is_connected():
                cursor.close()
                conn.close()
    
    def update_job_status(
        self,
        job_id: str,
        status: str,
        output_path: Optional[str] = None  # ä¿ç•™åƒæ•¸ç›¸å®¹æ€§ï¼Œä½†ä¸å†ä½¿ç”¨
    ) -> bool:
        """
        æ›´æ–°ä»»å‹™ç‹€æ…‹
        
        Args:
            job_id: ä»»å‹™ ID
            status: æ–°ç‹€æ…‹ (finished, failed, cancelled)
            output_path: [å·²æ£„ç”¨] è¼¸å‡ºè·¯å¾‘ï¼ˆä¸å†å„²å­˜ï¼Œæ”¹ç”¨ ID æ¨å°ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        sql = "UPDATE jobs SET status = %s WHERE id = %s"
        params = (status, job_id)
        
        try:
            conn = self.pool.get_connection()
            cursor = conn.cursor()
            cursor.execute(sql, params)
            conn.commit()
            logger.info(f"âœ“ ä»»å‹™ç‹€æ…‹æ›´æ–°: {job_id} -> {status}")
            return True
        except Error as e:
            logger.error(f"âœ— æ›´æ–°ä»»å‹™ç‹€æ…‹å¤±æ•—: {e}")
            return False
        finally:
            if conn.is_connected():
                cursor.close()
                conn.close()
    
    def get_history(
        self,
        limit: int = 50,
        offset: int = 0,
        include_deleted: bool = False,
        user_id: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        ç²å–æ­·å²è¨˜éŒ„
        
        Args:
            limit: è¿”å›æ•¸é‡
            offset: åç§»é‡
            include_deleted: æ˜¯å¦åŒ…å«å·²åˆªé™¤è¨˜éŒ„
            user_id: ç”¨æˆ¶ ID (Member System éæ¿¾)
        
        Returns:
            ä»»å‹™è¨˜éŒ„åˆ—è¡¨
        """
        where_clauses = []
        params = []
        
        if not include_deleted:
            where_clauses.append("deleted_at IS NULL AND is_deleted = FALSE")
        
        if user_id is not None:
            where_clauses.append("user_id = %s")
            params.append(user_id)
        
        where_clause = "WHERE " + " AND ".join(where_clauses) if where_clauses else ""
        
        sql = f"""
        SELECT id, user_id, prompt, workflow_name as workflow, model, aspect_ratio, batch_size, seed,
               status, created_at, updated_at
        FROM jobs
        {where_clause}
        ORDER BY created_at DESC
        LIMIT %s OFFSET %s
        """
        params.extend([limit, offset])
        
        conn = None
        cursor = None
        try:
            conn = self.pool.get_connection()
            cursor = conn.cursor(dictionary=True)
            
            logger.info(f"ğŸ” åŸ·è¡Œ SQL æŸ¥è©¢ (limit={limit}, offset={offset}, user_id={user_id})")
            cursor.execute(sql, tuple(params))
            results = cursor.fetchall()
            
            logger.info(f"ğŸ“Š fetchall() è¿”å› {len(results)} ç­†è¨˜éŒ„")
            
            # å°‡ datetime è½‰æ›ç‚º ISO å­—ä¸²
            for row in results:
                if row.get('created_at'):
                    row['created_at'] = row['created_at'].isoformat()
                if row.get('updated_at'):
                    row['updated_at'] = row['updated_at'].isoformat()
                
                # æ§‹å»ºè¼¸å‡ºè·¯å¾‘ (ä½¿ç”¨ ID æ¨å°)
                row['output_path'] = f"/outputs/{row['id']}.png"
            
            return results
        except Error as e:
            logger.error(f"âœ— æŸ¥è©¢æ­·å²å¤±æ•—: {e}", exc_info=True)
            return []
        finally:
            if cursor:
                cursor.close()
            if conn and conn.is_connected():
                conn.close()
    
    def soft_delete_job(self, job_id: str) -> bool:
        """
        è»Ÿåˆªé™¤ä»»å‹™ (è¨­ç½® deleted_at)
        
        Args:
            job_id: ä»»å‹™ ID
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        sql = "UPDATE jobs SET deleted_at = CURRENT_TIMESTAMP, is_deleted = TRUE WHERE id = %s"
        
        try:
            conn = self.pool.get_connection()
            cursor = conn.cursor()
            cursor.execute(sql, (job_id,))
            conn.commit()
            logger.info(f"âœ“ ä»»å‹™å·²è»Ÿåˆªé™¤: {job_id}")
            return True
        except Error as e:
            logger.error(f"âœ— è»Ÿåˆªé™¤å¤±æ•—: {e}")
            return False
        finally:
            if conn.is_connected():
                cursor.close()
                conn.close()
    
    def get_or_create_user_id(self, ip_address: str) -> int:
        """
        æ ¹æ“š IP åœ°å€ç²å–æˆ–å»ºç«‹ç”¨æˆ¶ ID
        
        Args:
            ip_address: ç”¨æˆ¶çš„ IP åœ°å€
        
        Returns:
            ç”¨æˆ¶ ID (INT)
        """
        try:
            conn = self.pool.get_connection()
            cursor = conn.cursor(dictionary=True)
            
            query_sql = "SELECT id FROM user_mapping WHERE ip_address = %s"
            cursor.execute(query_sql, (ip_address,))
            result = cursor.fetchone()
            
            if result:
                update_sql = "UPDATE user_mapping SET last_active = CURRENT_TIMESTAMP WHERE ip_address = %s"
                cursor.execute(update_sql, (ip_address,))
                conn.commit()
                return result['id']
            else:
                insert_sql = "INSERT INTO user_mapping (ip_address) VALUES (%s)"
                cursor.execute(insert_sql, (ip_address,))
                conn.commit()
                user_id = cursor.lastrowid
                logger.debug(f"âœ“ æ–°ç”¨æˆ¶å»ºç«‹: User #{user_id} ({ip_address})")
                return user_id
        except Error as e:
            logger.error(f"âœ— ç²å–æˆ–å»ºç«‹ç”¨æˆ¶ ID å¤±æ•—: {e}")
            return -1
        finally:
            if conn.is_connected():
                cursor.close()
                conn.close()
    
    def get_active_users_count(self) -> int:
        """ç²å–éå» 24 å°æ™‚å…§æ´»èºçš„ç”¨æˆ¶æ•¸"""
        try:
            conn = self.pool.get_connection()
            cursor = conn.cursor()
            sql = "SELECT COUNT(*) FROM user_mapping WHERE last_active >= DATE_SUB(NOW(), INTERVAL 24 HOUR)"
            cursor.execute(sql)
            result = cursor.fetchone()
            return result[0] if result else 0
        except Error as e:
            logger.error(f"âœ— æŸ¥è©¢æ´»èºç”¨æˆ¶å¤±æ•—: {e}")
            return 0
        finally:
            if conn.is_connected():
                cursor.close()
                conn.close()
    
    def check_connection(self) -> bool:
        """æª¢æŸ¥è³‡æ–™åº«é€£æ¥æ˜¯å¦æ­£å¸¸"""
        try:
            conn = self.pool.get_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            cursor.fetchone()
            return True
        except Error as e:
            logger.error(f"âœ— è³‡æ–™åº«é€£æ¥æª¢æŸ¥å¤±æ•—: {e}")
            return False
        finally:
            if conn.is_connected():
                cursor.close()
                conn.close()
