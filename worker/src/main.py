"""
Worker Main Loop
=================
å¾ Redis ä½‡åˆ—å–å¾—ä»»å‹™ï¼Œè§£æ workflowï¼Œé€äº¤ ComfyUI åŸ·è¡Œã€‚
"""

import os
import sys
import json
import time
import redis
import base64
import uuid
import logging
import threading
from logging.handlers import RotatingFileHandler
from pathlib import Path
from datetime import datetime, timedelta

# ============================================
# æ·»åŠ  shared æ¨¡çµ„è·¯å¾‘
# ============================================
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# é…ç½®æ—¥èªŒç³»çµ± (å„ªå…ˆè¨­ç½®)
log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# ç¢ºä¿ logs ç›®éŒ„å­˜åœ¨
log_dir = Path(__file__).parent.parent.parent / 'logs'
log_dir.mkdir(parents=True, exist_ok=True)
log_file = log_dir / 'worker.log'

# é…ç½® RotatingFileHandler (5MB, ä¿ç•™ 3 ä»½)
file_handler = RotatingFileHandler(
    str(log_file),
    maxBytes=5*1024*1024,  # 5MB
    backupCount=3,
    encoding='utf-8'
)
file_handler.setFormatter(log_formatter)
file_handler.setLevel(logging.INFO)

# é…ç½®æ§åˆ¶å°è¼¸å‡º
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)
console_handler.setLevel(logging.INFO)

# é…ç½® root logger
logging.basicConfig(
    level=logging.INFO,
    handlers=[file_handler, console_handler]
)

logger = logging.getLogger(__name__)
logger.info("=" * 60)
logger.info("Worker æ—¥èªŒç³»çµ±å·²å•Ÿå‹•")
logger.info(f"æ—¥èªŒæª”æ¡ˆä½ç½®: {log_file}")
logger.info("=" * 60)

# ä½¿ç”¨å…±ç”¨çš„ load_env
from shared.utils import load_env
load_env()

from json_parser import parse_workflow
from comfy_client import ComfyClient
from config import (
    REDIS_HOST, REDIS_PORT, REDIS_PASSWORD,
    COMFYUI_INPUT_DIR, JOB_QUEUE, TEMP_FILE_MAX_AGE_HOURS,
    JOB_STATUS_EXPIRE_SECONDS, STORAGE_INPUT_DIR, print_config,
    WORKER_TIMEOUT
)


def get_redis_client() -> redis.Redis:
    """
    å»ºç«‹ Redis é€£æ¥
    """
    return redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        password=REDIS_PASSWORD,
        decode_responses=True
    )


def save_base64_image(base64_data: str, job_id: str, field_name: str) -> str:
    """
    å°‡ base64 åœ–ç‰‡ä¿å­˜åˆ° ComfyUI input ç›®éŒ„
    
    Args:
        base64_data: base64 ç·¨ç¢¼çš„åœ–ç‰‡æ•¸æ“š (å¯èƒ½åŒ…å« data:image/xxx;base64, å‰ç¶´)
        job_id: ä»»å‹™ ID
        field_name: æ¬„ä½åç¨± (source, target, input ç­‰)
    
    Returns:
        ä¿å­˜çš„æª”å (ä¸å«è·¯å¾‘ï¼Œç”¨æ–¼ ComfyUI ç›¸å°è·¯å¾‘åƒè€ƒ)
    """
    import io
    from PIL import Image
    
    # ç§»é™¤ data:image/xxx;base64, å‰ç¶´ï¼ˆæ›´åš´æ ¼çš„è™•ç†ï¼‰
    if isinstance(base64_data, str) and "," in base64_data:
        base64_data = base64_data.split(",", 1)[1].strip()
    
    # è§£ç¢¼ base64
    try:
        image_bytes = base64.b64decode(base64_data)
    except Exception as e:
        raise ValueError(f"Base64 è§£ç¢¼å¤±æ•—: {e}")
    
    # å§‹çµ‚è½‰æ›ç‚ºçœŸæ­£çš„ PNG æ ¼å¼ï¼ˆè§£æ±ºæ ¼å¼ä¸åŒ¹é…å•é¡Œï¼‰
    # åŸå› ï¼šç”¨æˆ¶ä¸Šå‚³çš„åœ–ç‰‡å¯èƒ½æ˜¯ JPEG, AVIF, WebP ç­‰æ ¼å¼ï¼Œ
    #       å¦‚æœç›´æ¥ä¿å­˜ç‚º .png å‰¯æª”åä½†å…§å®¹æ˜¯å…¶ä»–æ ¼å¼ï¼Œ
    #       ComfyUI çš„ LoadImage ç¯€é»å¯èƒ½ç„¡æ³•æ­£ç¢ºè­˜åˆ¥
    try:
        img = Image.open(io.BytesIO(image_bytes))
        original_format = img.format
        logger.info(f"ğŸ“· åŸå§‹åœ–ç‰‡æ ¼å¼: {original_format}, å°ºå¯¸: {img.size}")
        
        # è½‰æ›ç‚º RGBï¼ˆè™•ç† RGBA æˆ–å…¶ä»–è‰²å½©æ¨¡å¼ï¼‰
        if img.mode in ('RGBA', 'LA', 'P'):
            # å‰µå»ºç™½è‰²èƒŒæ™¯
            background = Image.new('RGB', img.size, (255, 255, 255))
            if img.mode == 'P':
                img = img.convert('RGBA')
            background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
            img = background
        elif img.mode != 'RGB':
            img = img.convert('RGB')
        
        # ä¿å­˜ç‚ºçœŸæ­£çš„ PNG æ ¼å¼
        img_buffer = io.BytesIO()
        img.save(img_buffer, format='PNG', optimize=True)
        image_bytes = img_buffer.getvalue()
        logger.info(f"âœ… å·²è½‰æ›ç‚º PNG æ ¼å¼ï¼Œå¤§å°: {len(image_bytes)} bytes")
        
    except Exception as e:
        raise ValueError(f"åœ–ç‰‡æ ¼å¼è½‰æ›å¤±æ•—: {e}")

    
    # ç”Ÿæˆå”¯ä¸€æª”åï¼ˆåªç”¨æª”åï¼Œä¸ç”¨çµ•å°è·¯å¾‘ï¼‰
    filename = f"upload_{job_id}_{field_name}.png"
    filepath = Path(COMFYUI_INPUT_DIR) / filename
    
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    # å¯«å…¥æª”æ¡ˆ
    try:
        with open(filepath, "wb") as f:
            f.write(image_bytes)
            f.flush()  # ç¢ºä¿å¯«å…¥ç£ç¢Ÿ
            os.fsync(f.fileno())  # å¼·åˆ¶åŒæ­¥åˆ°ç£ç¢Ÿ
        
        logger.info(f"ğŸ’¾ å·²ä¿å­˜åœ–ç‰‡: {filename} ({len(image_bytes)} bytes) è‡³ {filepath}")
        
        # é©—è­‰æª”æ¡ˆæ˜¯å¦å¯è®€ï¼ˆç¢ºä¿ ComfyUI èƒ½è®€å–ï¼‰
        if not filepath.exists():
            raise FileNotFoundError(f"æª”æ¡ˆå¯«å…¥å¾Œç„¡æ³•æ‰¾åˆ°: {filepath}")
        
        file_size = filepath.stat().st_size
        if file_size != len(image_bytes):
            raise IOError(f"æª”æ¡ˆå¤§å°ä¸ç¬¦ï¼Œé æœŸ {len(image_bytes)} bytesï¼Œå¯¦éš› {file_size} bytes")
        
        # å˜—è©¦é‡æ–°é–‹å•Ÿæª”æ¡ˆé©—è­‰å¯è®€æ€§
        try:
            from PIL import Image
            with Image.open(filepath) as test_img:
                test_img.verify()
            logger.info(f"âœ… æª”æ¡ˆé©—è­‰æˆåŠŸ: {filename}")
        except Exception as verify_err:
            logger.warning(f"âš ï¸ æª”æ¡ˆé©—è­‰è­¦å‘Š: {verify_err}")
        
        # æ·»åŠ çŸ­æš«å»¶é²ï¼Œç¢ºä¿æª”æ¡ˆç³»çµ±å®Œæˆæ‰€æœ‰ I/O æ“ä½œ
        time.sleep(0.1)
        
    except Exception as e:
        logger.error(f"âŒ æª”æ¡ˆå¯«å…¥å¤±æ•—: {e}")
        raise
    
    # è¿”å›åªæœ‰æª”åï¼ˆç›¸å°è·¯å¾‘ï¼‰ï¼Œä¸è¿”å›çµ•å°è·¯å¾‘
    return filename


def copy_audio_to_comfyui(audio_filename: str, job_id: str) -> str:
    """
    å°‡éŸ³è¨Šæª”æ¡ˆå¾ storage/inputs è¤‡è£½åˆ° ComfyUI input ç›®éŒ„
    
    Args:
        audio_filename: ä¸Šå‚³çš„éŸ³è¨Šæª”å (å¦‚ audio_1ba6e2ba-e8a.mp3)
        job_id: ä»»å‹™ ID (ç”¨æ–¼ç”Ÿæˆå”¯ä¸€æª”å)
    
    Returns:
        è¤‡è£½å¾Œçš„æª”å (ä¸å«è·¯å¾‘)
    """
    import shutil
    
    # ä¾†æºæª”æ¡ˆè·¯å¾‘
    source_path = Path(STORAGE_INPUT_DIR) / audio_filename
    
    if not source_path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°éŸ³è¨Šæª”æ¡ˆ: {source_path}")
    
    # ä¿ç•™åŸå‰¯æª”åï¼Œç”Ÿæˆæ–°æª”å
    file_ext = source_path.suffix.lower()
    new_filename = f"audio_{job_id}{file_ext}"
    dest_path = Path(COMFYUI_INPUT_DIR) / new_filename
    
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    dest_path.parent.mkdir(parents=True, exist_ok=True)
    
    # è¤‡è£½æª”æ¡ˆ
    shutil.copy2(source_path, dest_path)
    
    logger.info(f"ğŸµ å·²è¤‡è£½éŸ³è¨Š: {audio_filename} -> {new_filename} ({source_path.stat().st_size} bytes)")
    return new_filename


def cleanup_old_temp_files():
    """
    æ¸…ç†è¶…éæŒ‡å®šæ™‚é–“çš„æš«å­˜åœ–ç‰‡æª”æ¡ˆ
    """
    input_dir = Path(COMFYUI_INPUT_DIR)
    if not input_dir.exists():
        return
    
    cutoff_time = datetime.now() - timedelta(hours=TEMP_FILE_MAX_AGE_HOURS)
    deleted_count = 0
    
    for filepath in input_dir.glob("upload_*.png"):
        try:
            file_mtime = datetime.fromtimestamp(filepath.stat().st_mtime)
            if file_mtime < cutoff_time:
                filepath.unlink()
                deleted_count += 1
        except Exception as e:
            logger.warning(f"âš ï¸ ç„¡æ³•åˆªé™¤ {filepath}: {e}")
    
    if deleted_count > 0:
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç† {deleted_count} å€‹éæœŸæš«å­˜æª”æ¡ˆ")


def cleanup_old_output_files(db_client=None):
    """
    æ¸…ç† storage/outputs ä¸­è¶…é 30 å¤©çš„åœ–ç‰‡æª”æ¡ˆ
    ä¸¦åŒæ­¥è»Ÿåˆªé™¤è³‡æ–™åº«è¨˜éŒ„
    
    Args:
        db_client: Database å®¢æˆ¶ç«¯å¯¦ä¾‹ï¼ˆç”¨æ–¼åŒæ­¥è»Ÿåˆªé™¤ï¼‰
    """
    from config import STORAGE_OUTPUT_DIR
    
    if not STORAGE_OUTPUT_DIR.exists():
        return
    
    cutoff_time = datetime.now() - timedelta(days=30)
    deleted_count = 0
    total_size = 0
    db_synced = 0
    
    for filepath in STORAGE_OUTPUT_DIR.glob("*"):
        if not filepath.is_file():
            continue
        
        try:
            file_mtime = datetime.fromtimestamp(filepath.stat().st_mtime)
            if file_mtime < cutoff_time:
                file_size = filepath.stat().st_size
                filename = filepath.name
                
                # åˆªé™¤æª”æ¡ˆ
                filepath.unlink()
                deleted_count += 1
                total_size += file_size
                
                # åŒæ­¥è»Ÿåˆªé™¤è³‡æ–™åº«è¨˜éŒ„ (å¦‚æœæœ‰è³‡æ–™åº«é€£æ¥)
                if db_client:
                    try:
                        if db_client.soft_delete_by_output_path(filename):
                            db_synced += 1
                    except Exception as db_err:
                        logger.warning(f"âš ï¸ è³‡æ–™åº«è»Ÿåˆªé™¤å¤±æ•—: {db_err}")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ç„¡æ³•åˆªé™¤ {filepath}: {e}")
    
    if deleted_count > 0:
        size_mb = total_size / (1024 * 1024)
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç† {deleted_count} å€‹è¶…é 30 å¤©çš„è¼¸å‡ºåœ–ç‰‡ (é‡‹æ”¾ {size_mb:.2f} MB)")
        if db_client and db_synced > 0:
            logger.info(f"ğŸ“Š å·²åŒæ­¥è»Ÿåˆªé™¤è³‡æ–™åº«è¨˜éŒ„: {db_synced} ç­†")


def worker_heartbeat(redis_client):
    """
    Worker å¿ƒè·³ç·šç¨‹ - æ¯ 10 ç§’å‘ Redis ç™¼é€å¿ƒè·³ä¿¡è™Ÿ
    Backend å¯é€šéæª¢æŸ¥ 'worker:heartbeat' éµä¾†åˆ¤æ–· Worker æ˜¯å¦åœ¨ç·š
    
    Args:
        redis_client: Redis å®¢æˆ¶ç«¯å¯¦ä¾‹
    """
    while True:
        try:
            # è¨­ç½®å¿ƒè·³éµï¼Œ30 ç§’éæœŸ
            redis_client.setex('worker:heartbeat', 30, 'alive')
            logger.debug("ğŸ’“ Worker å¿ƒè·³ç™¼é€æˆåŠŸ")
            time.sleep(10)  # æ¯ 10 ç§’ç™¼é€ä¸€æ¬¡
        except Exception as e:
            logger.error(f"âŒ Worker å¿ƒè·³ç™¼é€å¤±æ•—: {e}")
            time.sleep(10)


def update_job_status(
    r: redis.Redis,
    job_id: str,
    status: str,
    progress: int = 0,
    image_url: str = None,
    error: str = None,
    db_client=None
):
    """
    æ›´æ–°ä»»å‹™ç‹€æ…‹åˆ° Redis å’Œ MySQL
    
    Args:
        r: Redis å®¢æˆ¶ç«¯
        job_id: ä»»å‹™ ID
        status: ç‹€æ…‹ (processing, finished, failed)
        progress: é€²åº¦ (0-100)
        image_url: è¼¸å‡ºåœ–ç‰‡ URL
        error: éŒ¯èª¤è¨Šæ¯
        db_client: Database å®¢æˆ¶ç«¯ (å¯é¸ï¼Œç”¨æ–¼åŒæ­¥åˆ° MySQL)
    """
    # 1. æ›´æ–° Redis
    status_key = f"job:status:{job_id}"
    data = {
        "status": status,
        "progress": progress
    }
    
    if image_url:
        data["image_url"] = image_url
    if error:
        data["error"] = error
    
    r.hset(status_key, mapping=data)
    r.expire(status_key, JOB_STATUS_EXPIRE_SECONDS)
    logger.info(f"âœ“ Redis ç‹€æ…‹æ›´æ–°: {job_id} -> {status}")
    
    # 2. åŒæ­¥åˆ° MySQL (å¦‚æœå¯ç”¨ä¸”ç‹€æ…‹ç‚º finished æˆ– failed)
    if db_client and status in ['finished', 'failed']:
        try:
            # è½‰æ› image_url ç‚º output_path (å»é™¤ /outputs/ å‰ç¶´)
            output_path = None
            if image_url:
                output_path = image_url.replace('/outputs/', '')
            
            success = db_client.update_job_status(
                job_id=job_id,
                status=status,
                output_path=output_path
            )
            if success:
                logger.info(f"âœ“ MySQL ç‹€æ…‹åŒæ­¥: {job_id} -> {status}")
            else:
                logger.warning(f"âš ï¸ MySQL ç‹€æ…‹åŒæ­¥å¤±æ•—: {job_id}")
        except Exception as e:
            logger.error(f"âŒ MySQL åŒæ­¥éŒ¯èª¤: {e}")



def process_job(r: redis.Redis, client: ComfyClient, job_data: dict, db_client=None):
    """
    è™•ç†å–®å€‹ä»»å‹™
    
    job_data æ ¼å¼:
    {
        "job_id": "uuid",
        "prompt": "æè¿°æ–‡å­—",
        "seed": -1,
        "workflow": "text_to_image",
        "model": "turbo_fp8",
        "aspect_ratio": "1:1",
        "batch_size": 1,
        "images": {
            "source": "base64...",
            "target": "base64..."
        }
    }
    """
    job_id = job_data.get("job_id", "unknown")
    
    logger.info("="*50)
    logger.info(f"ğŸš€ é–‹å§‹è™•ç†ä»»å‹™: {job_id}")
    logger.info("="*50)
    
    try:
        # 1. æ›´æ–°ç‹€æ…‹ç‚ºè™•ç†ä¸­
        update_job_status(r, job_id, "processing", progress=10, db_client=db_client)
        
        # 2. æå–åƒæ•¸
        workflow_name = job_data.get("workflow", "text_to_image")
        prompt = job_data.get("prompt", "")
        prompts = job_data.get("prompts", [])  # Veo3 Long Video: å¤šæ®µ prompts
        seed = job_data.get("seed", -1)
        aspect_ratio = job_data.get("aspect_ratio", "1:1")
        model = job_data.get("model", "turbo_fp8")
        batch_size = job_data.get("batch_size", 1)
        images = job_data.get("images", {})  # base64 åœ–ç‰‡å­—å…¸
        
        logger.info(f"Workflow: {workflow_name}")
        logger.info(f"Prompt: {prompt[:50] if prompt else '(empty)'}...")
        if prompts:
            logger.info(f"Prompts: {len(prompts)} segments")
        logger.info(f"Aspect Ratio: {aspect_ratio}")
        logger.info(f"Model: {model}")
        logger.info(f"Batch Size: {batch_size}")
        logger.info(f"Images: {list(images.keys()) if images else 'None'}")
        
        # 3. è™•ç†ä¸Šå‚³çš„åœ–ç‰‡ (base64 -> æª”æ¡ˆ)
        update_job_status(r, job_id, "processing", progress=15, db_client=db_client)
        
        image_files = {}  # å„²å­˜æª”åæ˜ å°„ {"source": "upload_xxx_source.png"}
        if images:
            logger.info(f"ğŸ“· é–‹å§‹è™•ç† {len(images)} å¼µåœ–ç‰‡...")
            for field_name, base64_data in images.items():
                if base64_data:
                    try:
                        filename = save_base64_image(base64_data, job_id, field_name)
                        image_files[field_name] = filename
                    except Exception as e:
                        logger.warning(f"âš ï¸ è™•ç†åœ–ç‰‡ {field_name} å¤±æ•—: {e}")
        
        # 3.5 è™•ç†éŸ³è¨Šåƒæ•¸ (Phase 7 æ–°å¢)
        # éœ€è¦å°‡éŸ³è¨Šå¾ storage/inputs è¤‡è£½åˆ° ComfyUI/input
        audio_file = job_data.get("audio", "")
        comfyui_audio_file = ""
        if audio_file:
            logger.info(f"ğŸµ Audio file specified: {audio_file}")
            try:
                comfyui_audio_file = copy_audio_to_comfyui(audio_file, job_id)
            except Exception as e:
                logger.warning(f"âš ï¸ è¤‡è£½éŸ³è¨Šæª”æ¡ˆå¤±æ•—: {e}")
                comfyui_audio_file = ""
        
        # 4. è§£æ workflow (åŒ…å«åœ–ç‰‡èˆ‡éŸ³è¨Šæ³¨å…¥)
        update_job_status(r, job_id, "processing", progress=20, db_client=db_client)
        
        workflow = parse_workflow(
            workflow_name=workflow_name,
            prompt=prompt,
            seed=seed,
            aspect_ratio=aspect_ratio,
            model=model,
            batch_size=batch_size,
            image_files=image_files,      # å‚³å…¥åœ–ç‰‡æª”åæ˜ å°„
            audio_file=comfyui_audio_file, # å‚³å…¥è¤‡è£½å¾Œçš„éŸ³è¨Šæª”å (Phase 7)
            prompts=prompts               # Veo3 Long Video: å‚³å…¥å¤šæ®µ prompts
        )
        
        logger.info("Workflow è§£æå®Œæˆ")
        
        # 5. æª¢æŸ¥ ComfyUI é€£æ¥
        if not client.check_connection():
            raise Exception("ç„¡æ³•é€£æ¥ ComfyUIï¼Œè«‹ç¢ºèªæ˜¯å¦å·²å•Ÿå‹•")
        
        # 6. æäº¤ä»»å‹™åˆ° ComfyUI
        update_job_status(r, job_id, "processing", progress=30, db_client=db_client)
        
        prompt_id = client.queue_prompt(workflow)
        if not prompt_id:
            raise Exception("ä»»å‹™æäº¤å¤±æ•—")
        
        logger.info(f"ä»»å‹™å·²æäº¤ï¼Œprompt_id: {prompt_id}")
        
        # 7. å®šç¾©é€²åº¦æ›´æ–°å›èª¿å‡½æ•¸
        def on_progress(progress):
            # æª¢æŸ¥ä»»å‹™æ˜¯å¦è¢«å–æ¶ˆ
            status_key = f"job:status:{job_id}"
            current_status = r.hget(status_key, "status")
            if current_status == "cancelled":
                logger.warning("ğŸ›‘ ä»»å‹™å·²è¢«å–æ¶ˆï¼Œç™¼é€ä¸­æ–·æŒ‡ä»¤...")
                client.interrupt()
                raise Exception("Task cancelled by user")
            
            # å°‡é€²åº¦å¾ 30% é–‹å§‹æ˜ å°„åˆ° 30-95%
            mapped_progress = 30 + int(progress * 0.65)
            update_job_status(r, job_id, "processing", progress=mapped_progress, db_client=db_client)

        # 8. ç­‰å¾… ComfyUI åŸ·è¡Œå®Œæˆ
        result = client.wait_for_completion(
            prompt_id=prompt_id,
            timeout=WORKER_TIMEOUT,  # ä½¿ç”¨é…ç½®å€¼ (é è¨­ 2400 ç§’ = 40 åˆ†é˜)
            on_progress=on_progress
        )

        # 9. æ ¹æ“šåŸ·è¡Œçµæœè™•ç†è¼¸å‡º
        if result.get("success"):
            videos = result.get("videos", [])
            gifs = result.get("gifs", [])  # VHS_VideoCombine è¼¸å‡ºå½±ç‰‡ä¹Ÿåœ¨é€™è£¡
            images = result.get("images", [])
            
            # åˆä½µæ‰€æœ‰è¦–è¨Šé¡è¼¸å‡º (videos + gifs)ï¼Œçµ±ä¸€è™•ç†
            all_video_outputs = []
            for v in videos:
                v["_source"] = "videos"
                all_video_outputs.append(v)
            for g in gifs:
                g["_source"] = "gifs"
                all_video_outputs.append(g)
            
            logger.info(f"ğŸ“Š è¼¸å‡ºçµ±è¨ˆ: videos={len(videos)}, gifs={len(gifs)}, images={len(images)}")
            
            output_list = []
            output_type = "unknown"
            
            # å„ªå…ˆé †åº: è¦–è¨Šé¡ (videos + gifs) > åœ–ç‰‡
            if all_video_outputs:
                output_list = all_video_outputs
                output_type = "video"
                logger.info(f"ğŸ¥ æ”¶åˆ° {len(all_video_outputs)} å€‹è¦–è¨Šè¼¸å‡º")
            elif images:
                output_list = images
                output_type = "image"
                logger.info(f"ğŸ“· æ”¶åˆ° {len(images)} å¼µè¼¸å‡ºåœ–ç‰‡")
            
            if output_list:
                # éæ¿¾æ‰è‡¨æ™‚é è¦½åœ–ï¼ˆtype: 'temp'ï¼‰ï¼Œåªä¿ç•™çœŸå¯¦è¼¸å‡º
                real_outputs = [item for item in output_list if item.get("type") != "temp"]
                
                if not real_outputs:
                    logger.warning("âš ï¸ åªæœ‰è‡¨æ™‚é è¦½åœ–ï¼Œæ²’æœ‰çœŸå¯¦è¼¸å‡º")
                    logger.info("ğŸ“‹ è‡¨æ™‚é è¦½åœ–åˆ—è¡¨:")
                    for item in output_list:
                        logger.info(f"   - {item.get('filename')} (type: {item.get('type')})")
                    # å¦‚æœå®Œå…¨æ²’æœ‰è¼¸å‡ºï¼Œä½¿ç”¨è‡¨æ™‚é è¦½åœ–ä½œç‚ºå¾Œå‚™
                    real_outputs = output_list
                else:
                    logger.info(f"âœ“ éæ¿¾å¾Œå‰©é¤˜ {len(real_outputs)} å€‹çœŸå¯¦è¼¸å‡º")
                
                # å„ªå…ˆé¸æ“‡å®Œæ•´åˆä½µçš„å½±ç‰‡ (filename åŒ…å« Combined æˆ– Full)
                selected_file = None
                
                # 1. ç¬¬ä¸€è¼ªç¯©é¸ï¼šæ‰¾ "Combined" æˆ– "Full" (Veo3 Long Video æœ€çµ‚è¼¸å‡º)
                for item in real_outputs:
                    filename = item.get("filename", "")
                    if "Combined" in filename or "Full" in filename:
                        selected_file = item
                        logger.info(f"âœ¨ å„ªå…ˆé¸æ“‡åˆä½µå½±ç‰‡: {filename}")
                        break
                
                # 2. ç¬¬äºŒè¼ªç¯©é¸ï¼šå¦‚æœæœ‰ subfolder (å‚™é¸)
                if not selected_file:
                    for item in real_outputs:
                        if item.get("subfolder"):
                            selected_file = item
                            logger.info(f"é¸æ“‡æœ‰å­ç›®éŒ„çš„æª”æ¡ˆ: {item.get('filename')} (subfolder: {item.get('subfolder')})")
                            break
                
                # 3. æœ€å¾Œæ‰‹æ®µï¼šä½¿ç”¨æœ€å¾Œä¸€å€‹ï¼ˆé€šå¸¸æœ€çµ‚è¼¸å‡ºåœ¨æœ€å¾Œï¼‰
                if not selected_file:
                    selected_file = real_outputs[-1]
                    logger.info(f"ä½¿ç”¨æœ€å¾Œä¸€å€‹æª”æ¡ˆ: {selected_file.get('filename')}")
                
                # å˜—è©¦è¤‡è£½é¸ä¸­çš„æª”æ¡ˆï¼ˆå‚³é file_typeï¼‰
                file_type = selected_file.get("type", "output")
                new_filename = client.copy_output_file(
                    filename=selected_file.get("filename"),
                    subfolder=selected_file.get("subfolder", ""),
                    file_type=file_type,
                    job_id=job_id
                )
                
                # å¦‚æœé¸ä¸­çš„æª”æ¡ˆè¤‡è£½å¤±æ•—ï¼Œå˜—è©¦å…¶ä»–æª”æ¡ˆ
                if not new_filename and len(real_outputs) > 1:
                    logger.warning("âš ï¸ ç¬¬ä¸€é¸æ“‡å¤±æ•—ï¼Œå˜—è©¦å…¶ä»–æª”æ¡ˆ...")
                    for item in real_outputs:
                        if item == selected_file:
                            continue
                        file_type = item.get("type", "output")
                        new_filename = client.copy_output_file(
                            filename=item.get("filename"),
                            subfolder=item.get("subfolder", ""),
                            file_type=file_type,
                            job_id=job_id
                        )
                        if new_filename:
                            logger.info(f"âœ“ æˆåŠŸè¤‡è£½å‚™é¸æª”æ¡ˆ: {item.get('filename')}")
                            break
                
                if new_filename:
                    # ç„¡è«–æ˜¯åœ–ç‰‡é‚„æ˜¯å½±ç‰‡ï¼Œéƒ½é€šé image_url æ¬„ä½å›å‚³ (å‰ç«¯æœƒæ ¹æ“šå‰¯æª”ååˆ¤æ–·)
                    file_url = f"/outputs/{new_filename}"
                    update_job_status(r, job_id, "finished", progress=100, image_url=file_url, db_client=db_client)
                    logger.info(f"âœ… ä»»å‹™å®Œæˆï¼Œè¼¸å‡º ({output_type}): {file_url}")
                else:
                    update_job_status(r, job_id, "finished", progress=100, db_client=db_client)
                    logger.warning("âš ï¸ ä»»å‹™å®Œæˆï¼Œä½†æ‰€æœ‰è¼¸å‡ºæª”æ¡ˆéƒ½ç„¡æ³•è¤‡è£½")
            else:
                update_job_status(r, job_id, "finished", progress=100, db_client=db_client)
                logger.info("âœ… ä»»å‹™å®Œæˆï¼Œä½†æ²’æœ‰è¼¸å‡ºæª”æ¡ˆ")
        else:
            error = result.get("error", "æœªçŸ¥éŒ¯èª¤")
            
            # è¶…æ™‚æƒ…æ³ç‰¹æ®Šè™•ç†ï¼šå˜—è©¦å¾ History API ç²å–éƒ¨åˆ†çµæœ
            if "è¶…æ™‚" in error or "timeout" in error.lower():
                logger.warning("âš ï¸ ä»»å‹™è¶…æ™‚ï¼Œå˜—è©¦ç²å–å·²å®Œæˆçš„è¼¸å‡º...")
                try:
                    partial_outputs = client.get_outputs_from_history(prompt_id)
                    all_partial = partial_outputs.get("videos", []) + partial_outputs.get("gifs", []) + partial_outputs.get("images", [])
                    if all_partial:
                        # æœ‰éƒ¨åˆ†è¼¸å‡ºï¼Œä¹Ÿè¤‡è£½åˆ° Gallery
                        new_filename = client.copy_output_file(
                            filename=all_partial[-1].get("filename"),
                            subfolder=all_partial[-1].get("subfolder", ""),
                            job_id=job_id
                        )
                        if new_filename:
                            file_url = f"/outputs/{new_filename}"
                            update_job_status(r, job_id, "failed", error=f"{error} (partial output saved)", image_url=file_url, db_client=db_client)
                            logger.info(f"âš ï¸ ä»»å‹™è¶…æ™‚ä½†å·²ä¿å­˜éƒ¨åˆ†è¼¸å‡º: {file_url}")
                            return
                except Exception as partial_err:
                    logger.warning(f"âš ï¸ ç²å–éƒ¨åˆ†è¼¸å‡ºå¤±æ•—: {partial_err}")
            
            update_job_status(r, job_id, "failed", error=error, db_client=db_client)
            logger.error(f"âŒ ä»»å‹™å¤±æ•—: {error}")
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ è™•ç†éŒ¯èª¤: {error_msg}")
        update_job_status(r, job_id, "failed", progress=0, error=error_msg, db_client=db_client)


def main():
    """
    Worker ä¸»è¿´åœˆ
    """
    logger.info("="*50)
    logger.info("ğŸš€ Worker å•Ÿå‹•ä¸­...")
    logger.info("="*50)
    
    # 1. é€£æ¥ Redis
    try:
        r = get_redis_client()
        r.ping()
        logger.info(f"âœ… Redis é€£æ¥æˆåŠŸ ({REDIS_HOST}:{REDIS_PORT})")
    except Exception as e:
        logger.error(f"âŒ Redis é€£æ¥å¤±æ•—: {e}")
        sys.exit(1)
    
    # 2. é€£æ¥è³‡æ–™åº« (å¯é¸)
    db_client = None
    try:
        # å˜—è©¦å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥è³‡æ–™åº«é…ç½®
        db_host = os.getenv("DB_HOST", "localhost")
        db_port = int(os.getenv("DB_PORT", 3306))
        db_user = os.getenv("DB_USER", "studio_user")
        db_password = os.getenv("DB_PASSWORD", "studio_password")
        db_name = os.getenv("DB_NAME", "studio_db")
        
        # å¾ shared æ¨¡çµ„å°å…¥ Database é¡
        from shared.database import Database
        
        db_client = Database(
            host=db_host,
            port=db_port,
            user=db_user,
            password=db_password,
            database=db_name
        )
        logger.info(f"âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ ({db_host}:{db_port}/{db_name})")
    except Exception as e:
        logger.warning(f"âš ï¸ è³‡æ–™åº«é€£æ¥å¤±æ•— (åŠŸèƒ½é™ç´š): {e}")
    
    # 3. åˆå§‹åŒ– ComfyUI å®¢æˆ¶ç«¯
    client = ComfyClient()
    
    # 4. æª¢æŸ¥ ComfyUI é€£æ¥
    if client.check_connection():
        logger.info("âœ… ComfyUI é€£æ¥æˆåŠŸ")
    else:
        logger.warning("âš ï¸ ComfyUI å°šæœªå•Ÿå‹•ï¼Œå°‡æŒçºŒç­‰å¾…...")
    
    # 5. æ¸…ç†èˆŠçš„æš«å­˜æª”æ¡ˆ
    logger.info("ğŸ—‘ï¸ æ¸…ç†éæœŸæš«å­˜æª”æ¡ˆ...")
    cleanup_old_temp_files()
    
    # 6. æ¸…ç†è¶…é 30 å¤©çš„è¼¸å‡ºåœ–ç‰‡ (ä¸¦åŒæ­¥è³‡æ–™åº«)
    logger.info("ğŸ—‘ï¸ æ¸…ç†è¶…é 30 å¤©çš„è¼¸å‡ºåœ–ç‰‡...")
    cleanup_old_output_files(db_client)
    
    # 7. å•Ÿå‹• Worker å¿ƒè·³ç·šç¨‹
    logger.info("ğŸ’“ å•Ÿå‹• Worker å¿ƒè·³ç·šç¨‹...")
    heartbeat_thread = threading.Thread(target=worker_heartbeat, args=(r,), daemon=True)
    heartbeat_thread.start()
    
    # 8. é–‹å§‹è™•ç†ä½‡åˆ—
    logger.info(f"\nç›£è½ä½‡åˆ—: {JOB_QUEUE}")
    logger.info(f"ComfyUI Input ç›®éŒ„: {COMFYUI_INPUT_DIR}")
    logger.info("ç­‰å¾…ä»»å‹™ä¸­...\n")
    
    last_cleanup_time = time.time()
    CLEANUP_INTERVAL = 3600  # æ¯å°æ™‚æ¸…ç†ä¸€æ¬¡
    
    while True:
        try:
            # å®šæœŸæ¸…ç†æš«å­˜æª”æ¡ˆå’Œè¼¸å‡ºåœ–ç‰‡
            if time.time() - last_cleanup_time > CLEANUP_INTERVAL:
                cleanup_old_temp_files()
                cleanup_old_output_files(db_client)
                last_cleanup_time = time.time()
            
            # BLPOP: é˜»å¡å¼å–å‡ºä»»å‹™ (è¶…æ™‚ 5 ç§’)
            result = r.blpop(JOB_QUEUE, timeout=5)
            
            if result:
                queue_name, job_json = result
                
                try:
                    job_data = json.loads(job_json)
                    process_job(r, client, job_data, db_client)
                except json.JSONDecodeError as e:
                    logger.error(f"JSON è§£æéŒ¯èª¤: {e}")
            
        except redis.ConnectionError as e:
            logger.error(f"Redis é€£æ¥ä¸­æ–·ï¼Œ5 ç§’å¾Œé‡è©¦: {e}")
            time.sleep(5)
            try:
                r = get_redis_client()
            except:
                pass
                
        except KeyboardInterrupt:
            logger.info("\næ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨é—œé–‰...")
            break
            
        except Exception as e:
            logger.error(f"æœªé æœŸéŒ¯èª¤: {e}")
            time.sleep(1)
    
    logger.info("å·²é—œé–‰")


if __name__ == '__main__':
    main()
