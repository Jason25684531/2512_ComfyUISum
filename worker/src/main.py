"""
Worker Main Loop
=================
å¾ Redis ä½‡åˆ—å–å¾—ä»»å‹™ï¼Œè§£æ workflowï¼Œé€äº¤ ComfyUI åŸ·è¡Œã€‚
"""

import os
import sys
import json
import time
import redis
import base64
import uuid
import logging
from logging.handlers import RotatingFileHandler
from pathlib import Path
from datetime import datetime, timedelta

# é…ç½®æ—¥èªŒç³»çµ± (å„ªå…ˆè¨­ç½®)
log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# ç¢ºä¿ logs ç›®éŒ„å­˜åœ¨
log_dir = Path(__file__).parent.parent.parent / 'logs'
log_dir.mkdir(parents=True, exist_ok=True)
log_file = log_dir / 'worker.log'

# é…ç½® RotatingFileHandler (5MB, ä¿ç•™ 3 ä»½)
file_handler = RotatingFileHandler(
    str(log_file),
    maxBytes=5*1024*1024,  # 5MB
    backupCount=3,
    encoding='utf-8'
)
file_handler.setFormatter(log_formatter)
file_handler.setLevel(logging.INFO)

# é…ç½®æ§åˆ¶å°è¼¸å‡º
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)
console_handler.setLevel(logging.INFO)

# é…ç½® root logger
logging.basicConfig(
    level=logging.INFO,
    handlers=[file_handler, console_handler]
)

logger = logging.getLogger(__name__)
logger.info("=" * 60)
logger.info("Worker æ—¥èªŒç³»çµ±å·²å•Ÿå‹•")
logger.info(f"æ—¥èªŒæª”æ¡ˆä½ç½®: {log_file}")
logger.info("=" * 60)

# è‡ªå‹•è¼‰å…¥ .env æª”æ¡ˆ
def load_env():
    env_path = Path(__file__).parent.parent.parent / ".env"
    if env_path.exists():
        with open(env_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    os.environ.setdefault(key.strip(), value.strip())
        logger.info(f"å·²è¼‰å…¥ .env æª”æ¡ˆ: {env_path}")

load_env()

from json_parser import parse_workflow
from comfy_client import ComfyClient
from config import (
    REDIS_HOST, REDIS_PORT, REDIS_PASSWORD,
    COMFYUI_INPUT_DIR, JOB_QUEUE, TEMP_FILE_MAX_AGE_HOURS,
    JOB_STATUS_EXPIRE_SECONDS, print_config
)


def get_redis_client() -> redis.Redis:
    """
    å»ºç«‹ Redis é€£æ¥
    """
    return redis.Redis(
        host=REDIS_HOST,
        port=REDIS_PORT,
        password=REDIS_PASSWORD,
        decode_responses=True
    )


def save_base64_image(base64_data: str, job_id: str, field_name: str) -> str:
    """
    å°‡ base64 åœ–ç‰‡ä¿å­˜åˆ° ComfyUI input ç›®éŒ„
    
    Args:
        base64_data: base64 ç·¨ç¢¼çš„åœ–ç‰‡æ•¸æ“š (å¯èƒ½åŒ…å« data:image/xxx;base64, å‰ç¶´)
        job_id: ä»»å‹™ ID
        field_name: æ¬„ä½åç¨± (source, target, input ç­‰)
    
    Returns:
        ä¿å­˜çš„æª”å (ä¸å«è·¯å¾‘)
    """
    # ç§»é™¤ data:image/xxx;base64, å‰ç¶´
    if "," in base64_data:
        base64_data = base64_data.split(",", 1)[1]
    
    # è§£ç¢¼ base64
    try:
        image_bytes = base64.b64decode(base64_data)
    except Exception as e:
        raise ValueError(f"Base64 è§£ç¢¼å¤±æ•—: {e}")
    
    # ç”Ÿæˆå”¯ä¸€æª”å
    filename = f"upload_{job_id}_{field_name}.png"
    filepath = Path(COMFYUI_INPUT_DIR) / filename
    
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    # å¯«å…¥æª”æ¡ˆ
    with open(filepath, "wb") as f:
        f.write(image_bytes)
    
    logger.info(f"ğŸ’¾ å·²ä¿å­˜åœ–ç‰‡: {filename} ({len(image_bytes)} bytes)")
    return filename


def cleanup_old_temp_files():
    """
    æ¸…ç†è¶…éæŒ‡å®šæ™‚é–“çš„æš«å­˜åœ–ç‰‡æª”æ¡ˆ
    """
    input_dir = Path(COMFYUI_INPUT_DIR)
    if not input_dir.exists():
        return
    
    cutoff_time = datetime.now() - timedelta(hours=TEMP_FILE_MAX_AGE_HOURS)
    deleted_count = 0
    
    for filepath in input_dir.glob("upload_*.png"):
        try:
            file_mtime = datetime.fromtimestamp(filepath.stat().st_mtime)
            if file_mtime < cutoff_time:
                filepath.unlink()
                deleted_count += 1
        except Exception as e:
            logger.warning(f"âš ï¸ ç„¡æ³•åˆªé™¤ {filepath}: {e}")
    
    if deleted_count > 0:
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç† {deleted_count} å€‹éæœŸæš«å­˜æª”æ¡ˆ")


def cleanup_old_output_files(db_client=None):
    """
    æ¸…ç† storage/outputs ä¸­è¶…é 30 å¤©çš„åœ–ç‰‡æª”æ¡ˆ
    ä¸¦åŒæ­¥è»Ÿåˆªé™¤è³‡æ–™åº«è¨˜éŒ„
    
    Args:
        db_client: Database å®¢æˆ¶ç«¯å¯¦ä¾‹ï¼ˆç”¨æ–¼åŒæ­¥è»Ÿåˆªé™¤ï¼‰
    """
    from config import STORAGE_OUTPUT_DIR
    
    if not STORAGE_OUTPUT_DIR.exists():
        return
    
    cutoff_time = datetime.now() - timedelta(days=30)
    deleted_count = 0
    total_size = 0
    db_synced = 0
    
    for filepath in STORAGE_OUTPUT_DIR.glob("*"):
        if not filepath.is_file():
            continue
        
        try:
            file_mtime = datetime.fromtimestamp(filepath.stat().st_mtime)
            if file_mtime < cutoff_time:
                file_size = filepath.stat().st_size
                filename = filepath.name
                
                # åˆªé™¤æª”æ¡ˆ
                filepath.unlink()
                deleted_count += 1
                total_size += file_size
                
                # åŒæ­¥è»Ÿåˆªé™¤è³‡æ–™åº«è¨˜éŒ„ (å¦‚æœæœ‰è³‡æ–™åº«é€£æ¥)
                if db_client:
                    try:
                        if db_client.soft_delete_by_output_path(filename):
                            db_synced += 1
                    except Exception as db_err:
                        logger.warning(f"âš ï¸ è³‡æ–™åº«è»Ÿåˆªé™¤å¤±æ•—: {db_err}")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ç„¡æ³•åˆªé™¤ {filepath}: {e}")
    
    if deleted_count > 0:
        size_mb = total_size / (1024 * 1024)
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç† {deleted_count} å€‹è¶…é 30 å¤©çš„è¼¸å‡ºåœ–ç‰‡ (é‡‹æ”¾ {size_mb:.2f} MB)")
        if db_client and db_synced > 0:
            logger.info(f"ğŸ“Š å·²åŒæ­¥è»Ÿåˆªé™¤è³‡æ–™åº«è¨˜éŒ„: {db_synced} ç­†")


def update_job_status(
    r: redis.Redis,
    job_id: str,
    status: str,
    progress: int = 0,
    image_url: str = None,
    error: str = None
):
    """
    æ›´æ–°ä»»å‹™ç‹€æ…‹åˆ° Redis
    """
    status_key = f"job:status:{job_id}"
    data = {
        "status": status,
        "progress": progress
    }
    
    if image_url:
        data["image_url"] = image_url
    if error:
        data["error"] = error
    
    r.hset(status_key, mapping=data)
    r.expire(status_key, JOB_STATUS_EXPIRE_SECONDS)
    logger.info(f"æ›´æ–°ç‹€æ…‹: {job_id} -> {status}")


def process_job(r: redis.Redis, client: ComfyClient, job_data: dict):
    """
    è™•ç†å–®å€‹ä»»å‹™
    
    job_data æ ¼å¼:
    {
        "job_id": "uuid",
        "prompt": "æè¿°æ–‡å­—",
        "seed": -1,
        "workflow": "text_to_image",
        "model": "turbo_fp8",
        "aspect_ratio": "1:1",
        "batch_size": 1,
        "images": {
            "source": "base64...",
            "target": "base64..."
        }
    }
    """
    job_id = job_data.get("job_id", "unknown")
    
    logger.info("="*50)
    logger.info(f"ğŸš€ é–‹å§‹è™•ç†ä»»å‹™: {job_id}")
    logger.info("="*50)
    
    try:
        # 1. æ›´æ–°ç‹€æ…‹ç‚ºè™•ç†ä¸­
        update_job_status(r, job_id, "processing", progress=10)
        
        # 2. æå–åƒæ•¸
        workflow_name = job_data.get("workflow", "text_to_image")
        prompt = job_data.get("prompt", "")
        seed = job_data.get("seed", -1)
        aspect_ratio = job_data.get("aspect_ratio", "1:1")
        model = job_data.get("model", "turbo_fp8")
        batch_size = job_data.get("batch_size", 1)
        images = job_data.get("images", {})  # base64 åœ–ç‰‡å­—å…¸
        
        logger.info(f"Workflow: {workflow_name}")
        logger.info(f"Prompt: {prompt[:50] if prompt else '(empty)'}...")
        logger.info(f"Aspect Ratio: {aspect_ratio}")
        logger.info(f"Model: {model}")
        logger.info(f"Batch Size: {batch_size}")
        logger.info(f"Images: {list(images.keys()) if images else 'None'}")
        
        # 3. è™•ç†ä¸Šå‚³çš„åœ–ç‰‡ (base64 -> æª”æ¡ˆ)
        update_job_status(r, job_id, "processing", progress=15)
        
        image_files = {}  # å„²å­˜æª”åæ˜ å°„ {"source": "upload_xxx_source.png"}
        if images:
            logger.info(f"ğŸ“· é–‹å§‹è™•ç† {len(images)} å¼µåœ–ç‰‡...")
            for field_name, base64_data in images.items():
                if base64_data:
                    try:
                        filename = save_base64_image(base64_data, job_id, field_name)
                        image_files[field_name] = filename
                    except Exception as e:
                        logger.warning(f"âš ï¸ è™•ç†åœ–ç‰‡ {field_name} å¤±æ•—: {e}")
        
        # 4. è§£æ workflow (åŒ…å«åœ–ç‰‡æ³¨å…¥)
        update_job_status(r, job_id, "processing", progress=20)
        
        workflow = parse_workflow(
            workflow_name=workflow_name,
            prompt=prompt,
            seed=seed,
            aspect_ratio=aspect_ratio,
            model=model,
            batch_size=batch_size,
            image_files=image_files  # å‚³å…¥åœ–ç‰‡æª”åæ˜ å°„
        )
        
        logger.info("Workflow è§£æå®Œæˆ")
        
        # 5. æª¢æŸ¥ ComfyUI é€£æ¥
        if not client.check_connection():
            raise Exception("ç„¡æ³•é€£æ¥ ComfyUIï¼Œè«‹ç¢ºèªæ˜¯å¦å·²å•Ÿå‹•")
        
        # 6. æäº¤ä»»å‹™åˆ° ComfyUI
        update_job_status(r, job_id, "processing", progress=30)
        
        prompt_id = client.queue_prompt(workflow)
        if not prompt_id:
            raise Exception("ä»»å‹™æäº¤å¤±æ•—")
        
        logger.info(f"ä»»å‹™å·²æäº¤ï¼Œprompt_id: {prompt_id}")
        
        # 7. å®šç¾©é€²åº¦æ›´æ–°å›èª¿å‡½æ•¸
        def on_progress(progress):
            # æª¢æŸ¥ä»»å‹™æ˜¯å¦è¢«å–æ¶ˆ
            status_key = f"job:status:{job_id}"
            current_status = r.hget(status_key, "status")
            if current_status == "cancelled":
                logger.warning("ğŸ›‘ ä»»å‹™å·²è¢«å–æ¶ˆï¼Œç™¼é€ä¸­æ–·æŒ‡ä»¤...")
                client.interrupt()
                raise Exception("Task cancelled by user")
            
            # å°‡é€²åº¦å¾ 30% é–‹å§‹æ˜ å°„åˆ° 30-95%
            mapped_progress = 30 + int(progress * 0.65)
            update_job_status(r, job_id, "processing", progress=mapped_progress)

        # 8. ç­‰å¾… ComfyUI åŸ·è¡Œå®Œæˆ
        result = client.wait_for_completion(
            prompt_id=prompt_id,
            timeout=600,
            on_progress=on_progress
        )

        # 9. æ ¹æ“šåŸ·è¡Œçµæœè™•ç†è¼¸å‡º
        if result.get("success"):
            images = result.get("images", [])
            if images:
                logger.info(f"ğŸ“· æ”¶åˆ° {len(images)} å¼µè¼¸å‡ºåœ–ç‰‡")
                
                # å„ªå…ˆé¸æ“‡æœ‰ subfolder çš„åœ–ç‰‡ï¼ˆæ­£å¼è¼¸å‡ºï¼‰ï¼Œå¦å‰‡ä½¿ç”¨ç¬¬ä¸€å¼µ
                selected_image = None
                for img in images:
                    if img.get("subfolder"):
                        selected_image = img
                        logger.info(f"é¸æ“‡æœ‰å­ç›®éŒ„çš„åœ–ç‰‡: {img.get('filename')} (subfolder: {img.get('subfolder')})")
                        break
                
                if not selected_image:
                    selected_image = images[0]
                    logger.info(f"ä½¿ç”¨ç¬¬ä¸€å¼µåœ–ç‰‡: {selected_image.get('filename')}")
                
                # å˜—è©¦è¤‡è£½é¸ä¸­çš„åœ–ç‰‡
                new_filename = client.copy_output_image(
                    filename=selected_image.get("filename"),
                    subfolder=selected_image.get("subfolder", ""),
                    job_id=job_id
                )
                
                # å¦‚æœé¸ä¸­çš„åœ–ç‰‡è¤‡è£½å¤±æ•—ï¼Œå˜—è©¦å…¶ä»–åœ–ç‰‡
                if not new_filename and len(images) > 1:
                    logger.warning("âš ï¸ ç¬¬ä¸€é¸æ“‡å¤±æ•—ï¼Œå˜—è©¦å…¶ä»–åœ–ç‰‡...")
                    for img in images:
                        if img == selected_image:
                            continue
                        new_filename = client.copy_output_image(
                            filename=img.get("filename"),
                            subfolder=img.get("subfolder", ""),
                            job_id=job_id
                        )
                        if new_filename:
                            logger.info(f"âœ“ æˆåŠŸè¤‡è£½å‚™é¸åœ–ç‰‡: {img.get('filename')}")
                            break
                
                if new_filename:
                    image_url = f"/outputs/{new_filename}"
                    update_job_status(r, job_id, "finished", progress=100, image_url=image_url)
                    logger.info(f"âœ… ä»»å‹™å®Œæˆï¼Œè¼¸å‡º: {image_url}")
                else:
                    update_job_status(r, job_id, "finished", progress=100)
                    logger.warning("âš ï¸ ä»»å‹™å®Œæˆï¼Œä½†æ‰€æœ‰è¼¸å‡ºåœ–ç‰‡éƒ½ç„¡æ³•è¤‡è£½")
            else:
                update_job_status(r, job_id, "finished", progress=100)
                logger.info("âœ… ä»»å‹™å®Œæˆï¼Œä½†æ²’æœ‰è¼¸å‡ºåœ–ç‰‡")
        else:
            error = result.get("error", "æœªçŸ¥éŒ¯èª¤")
            update_job_status(r, job_id, "failed", error=error)
            logger.error(f"âŒ ä»»å‹™å¤±æ•—: {error}")
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ è™•ç†éŒ¯èª¤: {error_msg}")
        update_job_status(r, job_id, "failed", progress=0, error=error_msg)


def main():
    """
    Worker ä¸»è¿´åœˆ
    """
    logger.info("="*50)
    logger.info("ğŸš€ Worker å•Ÿå‹•ä¸­...")
    logger.info("="*50)
    
    # 1. é€£æ¥ Redis
    try:
        r = get_redis_client()
        r.ping()
        logger.info(f"âœ… Redis é€£æ¥æˆåŠŸ ({REDIS_HOST}:{REDIS_PORT})")
    except Exception as e:
        logger.error(f"âŒ Redis é€£æ¥å¤±æ•—: {e}")
        sys.exit(1)
    
    # 2. é€£æ¥è³‡æ–™åº« (å¯é¸)
    db_client = None
    try:
        # å˜—è©¦å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥è³‡æ–™åº«é…ç½®
        db_host = os.getenv("DB_HOST", "localhost")
        db_port = int(os.getenv("DB_PORT", 3306))
        db_user = os.getenv("DB_USER", "studio_user")
        db_password = os.getenv("DB_PASSWORD", "studio_password")
        db_name = os.getenv("DB_NAME", "studio_db")
        
        # å‹•æ…‹å°å…¥ Database é¡
        sys.path.insert(0, str(Path(__file__).parent.parent.parent / "backend" / "src"))
        from database import Database
        
        db_client = Database(
            host=db_host,
            port=db_port,
            user=db_user,
            password=db_password,
            database=db_name
        )
        logger.info(f"âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ ({db_host}:{db_port}/{db_name})")
    except Exception as e:
        logger.warning(f"âš ï¸ è³‡æ–™åº«é€£æ¥å¤±æ•— (åŠŸèƒ½é™ç´š): {e}")
    
    # 3. åˆå§‹åŒ– ComfyUI å®¢æˆ¶ç«¯
    client = ComfyClient()
    
    # 4. æª¢æŸ¥ ComfyUI é€£æ¥
    if client.check_connection():
        logger.info("âœ… ComfyUI é€£æ¥æˆåŠŸ")
    else:
        logger.warning("âš ï¸ ComfyUI å°šæœªå•Ÿå‹•ï¼Œå°‡æŒçºŒç­‰å¾…...")
    
    # 5. æ¸…ç†èˆŠçš„æš«å­˜æª”æ¡ˆ
    logger.info("ğŸ—‘ï¸ æ¸…ç†éæœŸæš«å­˜æª”æ¡ˆ...")
    cleanup_old_temp_files()
    
    # 6. æ¸…ç†è¶…é 30 å¤©çš„è¼¸å‡ºåœ–ç‰‡ (ä¸¦åŒæ­¥è³‡æ–™åº«)
    logger.info("ğŸ—‘ï¸ æ¸…ç†è¶…é 30 å¤©çš„è¼¸å‡ºåœ–ç‰‡...")
    cleanup_old_output_files(db_client)
    
    # 7. é–‹å§‹è™•ç†ä½‡åˆ—
    logger.info(f"\nç›£è½ä½‡åˆ—: {JOB_QUEUE}")
    logger.info(f"ComfyUI Input ç›®éŒ„: {COMFYUI_INPUT_DIR}")
    logger.info("ç­‰å¾…ä»»å‹™ä¸­...\n")
    
    last_cleanup_time = time.time()
    CLEANUP_INTERVAL = 3600  # æ¯å°æ™‚æ¸…ç†ä¸€æ¬¡
    
    while True:
        try:
            # å®šæœŸæ¸…ç†æš«å­˜æª”æ¡ˆå’Œè¼¸å‡ºåœ–ç‰‡
            if time.time() - last_cleanup_time > CLEANUP_INTERVAL:
                cleanup_old_temp_files()
                cleanup_old_output_files(db_client)
                last_cleanup_time = time.time()
            
            # BLPOP: é˜»å¡å¼å–å‡ºä»»å‹™ (è¶…æ™‚ 5 ç§’)
            result = r.blpop(JOB_QUEUE, timeout=5)
            
            if result:
                queue_name, job_json = result
                
                try:
                    job_data = json.loads(job_json)
                    process_job(r, client, job_data)
                except json.JSONDecodeError as e:
                    logger.error(f"JSON è§£æéŒ¯èª¤: {e}")
            
        except redis.ConnectionError as e:
            logger.error(f"Redis é€£æ¥ä¸­æ–·ï¼Œ5 ç§’å¾Œé‡è©¦: {e}")
            time.sleep(5)
            try:
                r = get_redis_client()
            except:
                pass
                
        except KeyboardInterrupt:
            logger.info("\næ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨é—œé–‰...")
            break
            
        except Exception as e:
            logger.error(f"æœªé æœŸéŒ¯èª¤: {e}")
            time.sleep(1)
    
    logger.info("å·²é—œé–‰")


if __name__ == '__main__':
    main()
